\documentclass[12pt,letterpaper]{report}
\usepackage{fullpage}
% \usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyhdr}
% \usepackage{fancy}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{dsfont}

\usepackage{mathtools}

\usepackage{fontspec}

\setromanfont{PTSerif}[
    Path=./fonts/,
    Extension = .ttf,
    UprightFont = *-Regular,
    BoldFont = *-Bold,
    ItalicFont = *-Italic,
    BoldItalicFont = *-BoldItalic,
]

\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% - Define Colors
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{light-gray}{gray}{0.85}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \pagestyle{headings}
\pagestyle{fancy}
\fancyhf{}
% \usepackage[top=1.5cm, bottom=2cm, left=2.5cm, right=2.5cm, headheight=110pt,showframe]{geometry}
% \usepackage[top=1.5cm, bottom=2cm, left=2.5cm, right=2.5cm, headheight=110pt]{geometry}
\usepackage[top=1.5cm, bottom=2cm, left=2.5cm, right=2.5cm]{geometry}
% \usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
% \fancyhead[C]{\rule{.5\textwidth}{4\baselineskip}}% Add something BIG in the header
% \setlength{\headheight}{52pt}
% \chead{\rightmark}
\rfoot{\rightmark}
\lfoot{\thepage}

\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0.2pt}

\title{Probability Theory Notes}
\author{}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% - Some declarations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{theorem}{Theorem}[section]
\newtheorem{claim}[theorem]{Claim}              % Вимога
\newtheorem{proposition}[theorem]{Proposition}  % Твердження
\newtheorem{lemma}[theorem]{Lemma}              % Лема
\newtheorem{corollary}[theorem]{Corollary}      % Наслідок
\newtheorem{conjecture}[theorem]{Conjecture}    % Припущення
\newtheorem*{observation}{Observation}
\newtheorem*{example}{Example}
\newtheorem*{remark}{Remark}
\newtheorem{definition}{Definition}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
% \DeclarePairedDelimiter{\set}{\left\{}{\right\}}
\NewDocumentCommand{\set}{o m}{%
  % <code>
  \IfNoValueTF{#1}
    {\{#2\}}
    {\{#1 \mid #2\}}%
  % <code>
}
\DeclareMathOperator{\cov}{cov}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle
\tableofcontents
% \newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% - Contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Числові характеристики випадкових величин}

\section{Попередні зауваження}

Розглянемо дискретну випадкову величину $\xi$
\[
    \xi(\omega) = \sum_{i=1}^{n} x_i \mathds{1}_{A_i}(\omega)
\] 
\[
    \set{A_1, \dots, A_n} \text{ - повна група подій}
\] 
Проведено $n$ незалежних випробувань в кожному з якиx спостерігається 
\[
    \xi_n(\omega) = \sum_{i=1}^{m} x_i \cdot \mathds{1}_{A_i}^{n}(\omega)
\] 
Розглянемо
\[
    \hat{\xi} = \frac{\xi_1 + \dots + \xi_n(\omega)}{n} =
    \frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{m} x_j \cdot
    \mathds{1}_{A_j}^{i} (\omega) = 
\] 
\[
    = \sum_{i=1}^{m} x_j \cdot \frac{1}{n} \sum_{i=1}^{n} \mathds{1}_{A_j}^{i} (\omega)
\] 
\[
    \frac{1}{n} \sum_{i=1}^{n} \mathds{1}_{A_j}^{i}(\omega) \text{ - частота появи $A_j$ в $n$ випробуваннях } \rightarrow_{n \rightarrow \infty} P(A_j)
.\] 
\[
    \hat{\xi} = \frac{\xi_1 + \dots + \xi_n}{n} \to_{n\to\infty} \sum_{j=1}^{m} x_j \cdot P(A_j)
.\] 

Припустимо $\Omega [0,1]; \; \mathcal{F} = \mathcal{B}([0,1])$,
$P$ міра Лебега, $P((a,b]) = b-a$

для дискретної ймовірності:

\begin{gather*}
    S_1 = x_1 \cdot P(A_1) = x_1 \cdot |A_1| \\
    S_2 = x_2 \cdot P(A_2) = x_2 \cdot |A_2| \\
    S \sim \sum_{j=1}^{m} x_j P(A_j) \text{ - площа }
\end{gather*}

для неперевного випадку:
\[
    \hat{\xi} \sim \int\limits_{\Omega} \xi(\omega)P(d\omega)
.\] 

\section{Definition and examples of expected value}

Нехай $(\omega, \mathcal{F}, P)$ - ймовірністний простір. $\xi$ - випадкова величина на цьому просторі.

\begin{definition}
    Математичним сподіванням випадкової величини $\xi$ називається число
    \[
        M\xi = \int\limits_{\Omega} \xi(\omega) P(d\omega)
    .\] 
    \[
        \text{(expectation) } \;\; E \xi = \int\limits_{\Omega} \xi(\omega) P(d\omega)
    .\] 
\end{definition}

$\xi$ індукує міру $P_{\xi}$ на $\mathbb{R}$:
\[
    P_{\xi} ((a,b]) = F_{\xi}(b) - F_{\xi}(a)
.\] 
Заміна $\xi(\omega) = x$ приводить до інтеграла Лебега-Стілтьєса:

\[
    M\xi = \int\limits_{\mathbb{R}}^{} x P_{\xi}(dx)
.\]
Звідси маємо інтеграл Стілтьєса: 
\[
    M\xi = \int\limits_{\mathbb{R}}^{} x dF_{\xi}(x)
.\] 

Для дискретної випадкової величини $\xi$:
\begin{equation} \label{eq:discrete_expectation}
    E\xi = \sum_{i=1}^{\infty} x_{i} \cdot P(\xi = x_i)
\end{equation}
Якщо $\xi$ має щільність $f_{\xi}(x)$:
\begin{equation} \label{eq:continuous_expectation}
    E\xi = \int\limits_{\mathbb{R}}^{}x f_{x} (x) dx
\end{equation}

\begin{remark}
    It is considered that expectation exists if series \eqref{eq:discrete_expectation} or integral \eqref{eq:continuous_expectation} is absolutely convergent.
\end{remark}

\begin{example}
    If $A \in \mathcal{F} $ then $\xi(\omega) = \mathds{1}_{A}(\omega)$
\[
    E\xi = 0 \cdot P(\xi=0) + 1 \cdot P(\xi=1) = P(A)
.\] 
\end{example}
\begin{example}
    \[
        P(\xi = i) = \frac{1}{i (i+1)}, i=1, 2, \dots
    .\] 
    \[
        \sum_{i=1}^{\infty} i \cdot P(\xi=i) = \sum_{i=1}^{\infty} \frac{1}{i+1} = +\infty \Rightarrow
        E\xi \text{ does not exist}
    .\] 
\end{example}

\begin{example}
   \[
       \xi \sim U(a, b); \; f_{\xi}(x) = \frac{1}{b-a} \mathds{1}(x\in(a,b])
   .\] 
   \[
       E\xi = \frac{1}{b-a} \int\limits_{a}^{b}x dx = \frac{b^2 - a^2}{2(b-a)} = \frac{a+b}{2}
   .\] 
   For uniform distribution the expectation is the middle of the segment.
\end{example}
\begin{example}
    \[
        \xi \sim C(0,1); \; f_{\xi}(x) = \frac{1}{\pi (a+x^2)}
    .\] 
    Whereas $\int\limits_{-\infty}^{\infty}\frac{xdx}{\pi (1+x^2)}$ - divergent then $E\xi$ does not exist.
\end{example}



Let $g$ - Borel function. Then $g(\xi)$ - stochastic variable. For $Mg(\xi)$ have:
\[
    Eg(\xi) = \int\limits_{\Omega}^{} g(\xi(\omega)) P(d\omega) = \int\limits_{\mathbb{R}}^{} g(x) P_{\xi} (dx) = \int\limits_{\mathbb{R}}^{} g(x) dF_{\xi} (x) 
.\] 
For discrete stochastic variable:
\[
    Eg(\xi) = \sum_{i=1}^{+\infty} g(x_i) \cdot P(\xi = x_i)
.\] 
For absolutely continuous:
\[
    Eg(\xi) = \int\limits_{\mathbb{R}}^{} g(x) f_{\xi}(x)dx
.\] 

If $\xi = (\xi_1, \dots, \xi_n)$ with density $f_{\xi}(x_1, \dots, x_n), \; g : \mathbb{R}^{n} \to \mathbb{R} $ - Borel function.
\[
    Eg(\xi_1, \dots, \xi_n) = \int \dots \int\limits_{\mathbb{R}^n} g(x_1, \dots, x_n) f_{\xi}(x_1, \dots, x_n) dx_1 \dots dx_n
.\] 

\begin{theorem} Properties of expectation \\

    \begin{itemize}
        \item[1.] $ Ec = c, \; c = const $
    \item[2.] $ E(a\xi + b) = a \cdot E\xi + b, \; a,b = const $
    \item[3.] $ E(\xi_1 + \xi_2) = E\xi_1 + E\xi_2 $
    \item[4.] $ E[\xi_1 \cdot \xi_2] = E\xi_1 \cdot E\xi_2 $ \\
        $\xi_1, \xi_2$ are independent stochastic variables 
    \item[5.] $\xi \geq 0 \Rightarrow M\xi \geq 0$ \\
        $ \xi \leq \eta \Rightarrow E\xi \leq E\eta $
    \item [6.] $ |E\xi| \leq E|\xi| $
    \end{itemize}
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item[4.] Let $\xi_1, \xi_2$ - absolutely continuous stochastic variables with densities
            \[
                f_{\xi_1}(x), f_{\xi_2}(y)
            .\] 
            \begin{align*}
                E[\xi_1, \xi_2] = \iint\limits_{\mathbb{R}^2} x \cdot y \cdot f_{(\xi_1,\xi_2)}(x,y)dx dy = \\
                = \iint\limits_{\mathbb{R}^2} x \cdot y \cdot f_{\xi_1}(x) \cdot f_{\xi_2}(y) dx dy = \\
                = \int\limits_{\mathbb{R}}^{} x f_{\xi_1}(x)dx \int\limits_{\mathbb{R}}^{} y f_{\xi_2}(y) dy = E\xi_1 \cdot E\xi_2
            .\end{align*}
    \end{itemize}
\end{proof}
\begin{remark}
    For arbitrary number of stochastic variables:
    \[
        E(\xi_1 + \dots + \xi_n) = \sum_{i=1}^{n} E\xi_i
    .\] 
    \[
        E(\xi_1 \cdot \dots \cdot \xi_n) = \prod_{i=1}^{n} E\xi_i 
    .\] 
    for $\xi_1, \dots, \xi_n$ that are independent together.
\end{remark}

\begin{example}
    let $\xi \sim Bin(n,p); \;\; E\xi - ?$
    \[
        P(\xi = k) = C_{n}^{k} p^{k} (1-p)^{n-k} , \; k = \overline{0,n}
    .\] 
    \[
        M\xi = \sum_{k=0}^{n} k \cdot C_n^k p^k (1-p)^{n-k}
    .\] 
    Using:
    \[
        \xi = \sum_{i=1} ^{n} \xi_i \; \text{ where } \xi_i \sim B(p):
    .\] 
    \[
        P(\xi_i = 1) = p; P(\xi_i = 0) = 1-p
        M\xi_i = 1 \cdot p + 0 \cdot (1-p) = p
    .\] 
    Then:
    \[
    M\xi = \sum_{i=1}^{n} M\xi_i = n \cdot p
    .\] 
\end{example}

\section{Dispersion}
\begin{definition}
    Dispersion of stochastic variable is called a number
    \[
        \mathcal{D}\xi = M(\xi - M\xi)^2
    .\] 
\end{definition}
\begin{remark}
   \[
       \mathcal{D}\xi = M(\xi^2 - 2M\xi \cdot \xi + (M\xi)^2) = M\xi^2 - 2M\xi \cdot M\xi + (M\xi)^2 = M\xi^2 - (M\xi)^2
   .\]  
\end{remark}
\begin{equation}
    \mathcal{D}\xi = M\xi^2 - (M\xi)^2
\end{equation}

\begin{definition}
    Number $M\xi^2$ is called second momentum os stochastic variable $\xi$.
\end{definition}

\begin{example}
    \[
    .\] 
    \begin{gather*}
        \xi \sim B(p); \; M\xi = p; \\
        M\xi^2 = 1\cdot P(\xi=1) + 0\cdot P(\xi=0) = p
        \mathcal{D}\xi = p - p^2 = p(1-p)
    .\end{gather*}
\end{example}
\begin{example}
    \begin{gather*}
        \xi \sim U(a, b); \; M\xi = \frac{a + b}{2} \\
        M\xi^2 = \int\limits_{a}^{b}x^2 \frac{1}{b-a} dx = \frac{b^2-a^2}{3(b-a)} = \frac{a^2+ab+b^2}{3} \\
        \mathcal{D}\xi = \frac{a^2+ab+b^2}{3} - \frac{(a+b)^2}{4} = \frac{4(a^2+ab+b^2) - 3(a+b)^2}{12} = \frac{(b-a)^2}{12}
    \end{gather*}
\end{example}

\begin{theorem}[properties of dispersion]
    \begin{itemize}
        \item[1.] \[ \mathcal{D}\xi \geq 0 \]
            \[ \mathcal{D}\xi = 0 \iff \xi = c = const \]
        \item[2.] \[ \mathcal{D}(a\xi + b) = a^2 \cdot \mathcal{D}\xi \]
        \item[3.] If $\xi_1$ and $\xi_2$ are independent, then 
            \[ \mathcal{D}(\xi_1 + \xi_2) = \mathcal{D}\xi_1 + \mathcal{D}\xi_2 \]
    \end{itemize}
\end{theorem}

\begin{proof}
    \begin{itemize}
        \item[1.]
        \begin{gather*}
                \mathcal{D}\xi = M(\xi - M\xi)^2 \\
                (\xi - M\xi)^2 \geq 0 \Rightarrow M(\xi -M\xi)^2 \geq 0 \\
                M(\xi-M\xi)^2 = 0 \iff (\xi-M\xi)^2 = 0 \iff \xi = M\xi = const
        \end{gather*}
    \item[2.] 
        \begin{gather*}
            \mathcal{D}(a\xi+b) = M((a\xi + b) - m(a\xi+b))^2 = M(a\xi + b - aM\xi -b)^2 = M a^2(\xi-m\xi)^2 = \\
            = a^2 \cdot M(\xi - M\xi)^2 = a^2 \cdot \mathcal{D}\xi.
        \end{gather*}
    \item[3.]
        Let $\xi_1$ and $\xi_1$ independent.
        \begin{gather*}
            \mathcal{D}(\xi_1 + \xi_2) = M(\xi_1 + \xi_2 - M(\xi_1 + \xi_2))^2 = 
            M((\xi_1 - M\xi_1) + (\xi_2 - M\xi_2))^2 = \\
            = M((\xi_1 -M\xi_1)^2 + 2(\xi_1-M\xi_1)(\xi_2-M\xi_2) + (\xi_2-M\xi_2)^2) = \\
            = \mathcal{D}\xi_1 + 2\cdot M[(\xi_1 -M\xi_1)(\xi_2-M\xi_2)] + \mathcal{D}\xi_2. \\
            \text{$\xi_1, \xi_2$ independent } \Rightarrow \mathcal{D}\xi_1 +
            \mathcal{D}\xi_2 + 2M(\xi_1-M\xi_1) \cdot M(\xi_2 -M\xi_2) = \mathcal{D}\xi_1 + \mathcal{D}\xi_2.
        \end{gather*}
    \end{itemize}
\end{proof}

\begin{example}
    \begin{gather*}
        \xi \sim \text{Bin}(n,p); \; M\xi = n\cdot p;  \; \mathcal{D}\xi = ? \\
        M\xi^2 = \sum_{k=0}^{n} k^2 \cdot C_n^k \cdot p^k \cdot (1-p)^{n-k} \\
        \xi = \sum_{i=1}^{n}\xi_i, \; \xi_i \sim \text{B}(p), \; \xi_2, \dots, \xi_n \text{ - independent } \\
        \mathcal{D}\xi = \sum_{i=1}^{n} \mathcal{D}\xi_i = \sum_{i=1}^{n}p\cdot(1-p) = np(1-p).
    \end{gather*}
\end{example}

\begin{remark}
    \[
        M\xi = \underset{a}{\text{argmin}} M(\xi - a)^2
    .\] 
    \begin{gather*}
        M(\xi - a)^2 = M((\xi - M\xi) + (M\xi - a))^2 = \\
        M(\xi - M\xi)^2 + 2(M\xi -a)M(\xi-M\xi) + (M\xi-a)^2=\\
        \mathcal{D}\xi + (M\xi -a)^2 \geq \mathcal{D}\xi \\
        \text{moreover } M(\xi - a)^2 = \mathcal{D}\xi \iff (M\xi-a)^2 = 0 \\
        \Rightarrow a = M\xi.
    \end{gather*}
\end{remark}

\begin{example}
    Numerical characteristics of the main probability distributions
    \begin{itemize}
        \item[1.] $ \xi \sim B(p), M\xi = p, \mathcal{D}\xi = p(1-p) $ 
        \item[2.] $ \xi \sim \text{Bin}(n,p), M\xi = np, \;\; \mathcal{D}\xi = np(1-p)$
        \item[3.] $\xi \sim \text{Poiss}(\lambda)$
            \begin{gather*}
                M\xi = \sum_{k=0}^{\infty} k\cdot \frac{\lambda^k}{k!} e^{-\lambda} =
                e^{-\lambda} \cdot \sum_{k=1}^{\infty}\frac{\lambda^k}{(k-1)!} = 
                \lambda e^{-\lambda} \cdot \sum_{k=0}^{\infty} \frac{\lambda^k}{k!} =
                \lambda e^{-\lambda} \cdot e^{\lambda} =
                \lambda \\
                M\xi^2 = \sum_{k=0}^{\infty} k^2 \cdot \frac{\lambda^k}{k!} e^{-\lambda} =
                e^{-\lambda} \sum_{k=1}^{\infty}k\frac{\lambda ^{k}}{(k-1)!} = \\
                e^{-\lambda} \sum_{k=1}^{\infty}((k-1)+1) \cdot \frac{\lambda^k}{(k-1)!} =
                e^{-\lambda} (\lambda^2 \sum_{k=2}^{\infty}\frac{\lambda^{k-1}}{(k-2)!} +
                    \lambda \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!}) =\\
                = \lambda^2 e^{-\lambda} e^{\lambda} + \lambda \cdot e^{-\lambda} \cdot e^{\lambda} =
                \lambda^2 + \lambda \\
                \mathcal{D}\xi = \lambda^2 + \lambda - \lambda^2 = \lambda
            \end{gather*}
        \item[4.] $\xi \sim \text{Geom}(p): p(\xi = k) = (1-p)^{k}, \; k=0,1,\dots$
            \begin{gather*}
                M\xi = \sum_{k=0}^{\infty} k\cdot (1-p)^{k} \cdot p \\
                \sum_{k=0}^{\infty}(1-p)^{k} = \frac{1}{p} \\
                \sum_{k=0}^{\infty} k (1-p)^{k-1} = \frac{1}{p^2} \;\; \mid p\cdot (1-p) \\
                \sum_{k=0}^{\infty} k(1-p)^{k}\cdot p = \frac{1-p}{p}\\
                M\xi^2 = \sum_{k=0}^{\infty} k^2 (1-p)^{k}\cdot p \\
                \sum_{k=0}^{\infty} k(1-p)^{k} = \frac{1-p}{p^2}=\frac{1}{p^2} - \frac{1}{p} \\
            \sum_{k=0}^{\infty} k^2 (1-p)^{k-1} = \frac{2}{p^{3}} - \frac{1}{p^2} \;\; \mid (1-p)\cdot p \\
                \sum_{k=0}^{\infty} k^2(1-p)^{k}\cdot p = \frac{2(1-p)}{p^2} - \frac{1-p}{p} \\
                \mathcal{D} = \frac{2(1-p)}{p^2} - \frac{1-p}{p} - \left(\frac{1-p}{p}\right)^2 =
                \frac{2(1-p)}{p^2} - \frac{1-p}{p}\left(1+\frac{1-p}{p}\right) = \\
                = \frac{2(1-p)}{p^2} - \frac{1-p}{p^2} = \frac{1-p}{p^2}; \\
                \mathcal{D}\xi = \frac{1-p}{p^2}; \;\; M\xi = \frac{1-p}{p}
            \end{gather*}
        \item[5.] $\xi \sim U(a,b); \;\; M\xi = \frac{a+b}{2}; \;\; \mathcal{D}\xi = \frac{(b-a)^2}{12}$
        \item[6.] $\xi \sim \text{Exp}(\lambda): \; f_{\xi} (x) = \lambda e^{-\lambda x} \cdot \mathds{1}(x\geq 0) $
            \begin{gather*}
                M\xi = \int\limits_{0}^{\infty} x \lambda e^{-\lambda x} dx = -\int\limits_{0}^{\infty} x de^{-\lambda x} = -x \cdot e^{-\lambda x} \mid_{0}^{\infty} + \int\limits_{0}^{\infty} e^{-\lambda x} dx = \frac{1}{\lambda}.\\
                M\xi^2 = \int\limits_{0}^{\infty} x^2 \lambda e^{-\lambda x} dx = \int\limits_{0}^{\infty}x^2 de^{-\lambda x} = \int\limits_{0}^{\infty}2x e^{-\lambda x}dx = \frac{2}{\lambda} \int\limits_{0}^{\infty}x \lambda e^{-\lambda x} dx = \frac{2}{\lambda^2} \\
                \mathcal{D}\xi = \frac{2}{\lambda^2} - \frac{1}{\lambda^2} = \frac{1}{\lambda^2} 
                \;\; M\xi = \frac{1}{\lambda}
            \end{gather*}
        \item[7.] $\xi \sim \text{N}(a, \sigma^2); $
            \[ f_{\xi}(x) = \frac{1}{\sqrt{2\pi \sigma}}e^{-\frac{(x-a)^2}{2\sigma^2}} \]
            \begin{gather*}
                M\xi = a; \;\; \mathcal{D}\xi = \sigma^2 \\
                M\xi = \int\limits_{\mathbb{R}}^{}x \frac{1}{\sqrt{2\pi}\sigma} e^{\frac{(x-a)^2}{2\sigma^2}}dx  = \\
                \frac{x-a}{\sigma} = z\\
                dx = \sigma dz \\
            =  \frac{1}{\sqrt{2\pi}} \int\limits_{\mathbb{R}}^{} (\sigma z + a) e^{-z^2/2} dz =
            = \frac{\sigma}{\sqrt{2\pi}} \cdot \int\limits_{\mathbb{R}}^{} z e^{-z^2/2} dz + 
                \frac{a}{\sqrt{2\pi}} \int\limits_{\mathbb{R}}^{} e^{-z^2/2} dz = a
            \end{gather*}
            \begin{gather*}
                \mathcal{D}\xi = M(\xi -M\xi)^2 = M(\xi -a)^2 =
                \int\limits_{\mathbb{R}}^{} (x-a)^2 \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-a)^2}{2\sigma^2}}dx = \\
                \frac{x-a}{\sigma} = z \\
                dx = \sigma dz \\
                = \frac{1}{\sqrt{2\pi}} \int\limits_{\mathbb{R}}^{} \sigma^2 z^2 \cdot e^{-z^2/2} dz =
                \frac{2\sigma^2}{\sqrt{2\pi}} \int\limits_{0}^{\infty} z^2 e^{-z^2/2} dz =
                - \frac{2\sigma^2}{\sqrt{2\pi}} \int\limits_{0}^{\infty} z de^{-z^2/2} = \\
                = -\frac{2\sigma^2}{\sqrt{2\pi}} z \cdot e^{-z^2/2} \mid_0^{\infty} +
                    \frac{2\sigma^2}{\sqrt{2\pi}} \int\limits_{0}^{\infty} e^{-z^2/2} dz =
                \sigma^2\cdot \frac{2}{\sqrt{2\pi}} \int\limits_{0}^{\infty} e^{-z^2/2}dz = \\
                = \sigma^2 \cdot \frac{1}{\sqrt{2\pi}} \int\limits_{-\infty}^{+\infty} e^{-z^2/2}dz = \sigma^2
            \end{gather*}
        \end{itemize}
\end{example}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   Covariance of random variables. Correlation coefficient.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Covariance of random variables. Correlation coefficient.}

\section{Covariance of random variables}

Consider $\xi = (\xi_1, \xi_2)$ - random vector.

\begin{definition}
    Covariation of stochastic variables $\xi_1, \xi_2$ is a number:
    \begin{equation}
        \text{cov}(\xi_1, \xi_2) = M[(\xi_1 -M\xi_1) \cdot(\xi_2 - M-\xi_2)]
    \end{equation}
    (assuming that $M\xi_i$ exist)

\end{definition}

If $\xi_1, \xi_2$ are discrete random variables, then
\begin{equation}
    \text{cov}(\xi_1, \xi_2) = \sum_{i=1}^{\infty}\sum_{j=1}^{\infty} (x_i - M\xi_1) \cdot (y_j - M\xi_2) \cdot P(\xi_1 = x_i, \xi_2 = y_j).
\end{equation}
If $\xi_1,\xi_2$ have common distribution density $f_{\xi}(x,y)$, then 
\begin{equation}
    \text{cov}(\xi_1,\xi_2) = \underset{\mathbb{R}^2}{\int\int}(x-M\xi_1)(y-M\xi_2) f_{\xi}(x,y)dxdy
\end{equation}
From definition Rightarrow:
\begin{gather*}
    \text{cov}(\xi_1, \xi_2) = M[\xi_1\cdot\xi_2 - \xi_1\cdot M\xi_2 - \xi_2\cdot M\xi_1 + M\xi_1 \cdot M\xi_2] = \\
    M[\xi_1\cdot\xi_2] - M\xi_2\cdot M\xi_1 - M\xi_1\cdot M\xi_2 + M\xi_1 \cdot M\xi_2 =
    M[\xi_1 \cdot \xi_2] - M\xi_1 \cdot M\xi_2.
\end{gather*}
\begin{equation}
    \text{cov}(\xi_1,\xi_2) = M[\xi_1\cdot\xi_2] - M\xi_1\cdot M\xi_2
\end{equation}

\begin{proposition}
    If $\xi_1, \xi_2$ are independent, then
    \[
        \text{cov}(\xi_1, \xi_2) = 0
    .\] 
    It is said, that $\xi_1,\xi_2$ are uncorrelated.

    Indeed, if $\xi_1, \xi_2$ are independent, then from the properties of expectation:
    \[
        M[\xi_1\cdot\xi_2] = M\xi_1\cdot M\xi_2
    .\] 
    then 
    \[
        \text{cov}(\xi_1,\xi_2) = M\xi_1\cdot M\xi_2 - M\xi_1\cdot M\xi_2 = 0
    .\] 
    Inverse statement is not true: from uncorrelated does not Rightarrow independency.
\end{proposition}

\begin{remark}
    \[
        \mathcal{D}(\xi_1 + \xi_2) = \mathcal{D}\xi_1 + \mathcal{D}\xi_2 + 2M(\xi_1 - M\xi_1)(\xi_2 -M\xi_2) = \mathcal{D}\xi_1 + \mathcal{D}\xi_2 + 2\operatorname{cov}(\xi_1, \xi_2)
    .\] 
\end{remark}

\begin{theorem}
    Main properties of variation:
    \begin{itemize}
        \item[1.] \[ \cov(\xi, \xi) = \mathcal{D}\xi \]
        \item[2.] \[ \cov(a_1 \xi_2 + b_1, a_2 \xi_2 + b_2) = a_1 \cdot a_2 \cov(\xi_1, \xi_2) \]
        \item[3.] \[ |\cov(\xi_1, \xi_2)| \leq \sqrt{\mathcal{D}\xi_1\cdot \mathcal{D}\xi_2} \]
        \item[4.] Equality $|\cov(\xi_1, \xi_2)| = \sqrt{\mathcal{D}\xi_1 \cdot \mathcal{D}\xi_2}$ is true if and only if $\xi_1$ and $\xi_2$ and linearly dependent.
            \[
                \exists a, b = const : \xi_2 = a \xi_2 - b
            .\] 
    \end{itemize}
    \begin{proof}
        \begin{enumerate}
            \item
                \begin{gather*}
                    \cov(\xi, \xi) = M[\xi \cdot \xi] - M\xi \cdot M\xi = M\xi^2 - (M\xi)^2 = \mathcal{D}\xi
                \end{gather*}
            \item
                \begin{gather*}
                    \cov(a_1\xi_1 + b_1, a_2\xi_2 + b_2) = M(a_1\xi_2 + b_1 - (a_1 M\xi_1 + b_1)) \cdot (a_2\xi_2+b_2-(a_2M\xi_2+b_2)) = \\
                    = M(a_1(\xi_1-M\xi_1)\cdot a_2(\xi_2 -M\xi_2)) = 
                    a_1\cdot a_2 \cdot M((\xi_1 -M\xi_1)(\xi_2 - M\xi_2)) = \\
                    = a_1 \cdot a_2\cdot \cov(\xi_1, \xi_2)
                \end{gather*}
            \item Consider stochastic variable:
                \[ \eta(x) = x\cdot \xi_1 - \xi_2, \;\; x\in \mathbb{R} \]
                \[ \mathcal{D}\eta(x) = \mathcal{D}(x\xi_1 - \xi_2) = x^2 \cdot \mathcal{D}\xi_1 + \mathcal{D}\xi_2 - 2x \cdot \cov(\xi_1,  \xi_2) \]
                As $\mathcal{D}\eta(x) \geq 0 \;\; \forall x \in \mathbb{R}$, so discriminant in the right part is not positive.
                \[ \mathcal{D} = (2\cov(\xi_1, \xi_2))^2 - 4disp\xi_1\cdot \mathcal{D}\xi_2 \leq 0 \]
                \[  \Rightarrow |\cov(\xi_1, \xi_2)| \leq \sqrt{\mathcal{D}\xi_1 \cdot \mathcal{D}\xi_2} \]
            \item 
                \begin{gather*}
                    |\cov(\xi_1, \xi_2)| = \sqrt{\mathcal{D}\xi_1\cdot \mathcal{D}\xi_2} \iff \mathcal{D} = 0 \iff \text{ equation $\mathcal{D}\eta(x)=0$ has solution $a$ } \iff \\
                    \iff \mathcal{D}\eta(a) = 0 \iff \eta(a) = b = const \iff
                    a\xi_1 - \xi_2 = b \iff \xi_2 = a\xi_1 - b.
                \end{gather*}
        \end{enumerate}
    \end{proof}

    \begin{remark}
        Covariation of stochastic variables shows how much their dependency is close to linear.
    \end{remark}
\end{theorem}

\section{Correlation coefficient}

\begin{definition}
    Correlation coefficient of random variables $\xi_1, \xi_2$ is a number:
    \[ \rho(\xi_1, \xi_2) = \frac{\cov(\xi_1, \xi_2)}{\sqrt{\mathcal{D}\xi_1 \cdot \mathcal{D}\xi_2}} \]
    considering that $\mathcal{D}\xi_i > 0$.
\end{definition}

\begin{theorem}
    Properties of covariation coefficient:
    \begin{enumerate}
        \item $ \rho(\xi,\xi) = 1 $
        \item $\xi_1$ and $\xi_2$ are independent and $\mathcal{D}\xi_i > 0 \Rightarrow \rho (\xi_1, \xi_2) = 0$ 
        \item $ |\rho(\xi_1, \xi_2)| = 1 \Rightarrow \xi_1$ and $\xi_2$ have linear dependency:
            \[ \xi_2 = a \xi_1 - b \]
            for any constants $a, b$.
        \item $\rho (a_1\xi_1 + b_1, a_2\xi_2 + b_2) = \pm \rho(\xi_1, \xi_2) = $
            \[ = \begin{cases}
                \rho(\xi_1, \xi_2) & a_1 \cdot a_2 > 0 \\
                -\rho(\xi_1, \xi_2) & a_1\cdot a_2 < 0
            \end{cases}\]
    \end{enumerate}
\end{theorem}
\begin{proof}
    DO IT YOURSELF
\end{proof}

\begin{example}
    Let $\xi_1, \xi_2$ air temperature of some two consistent days of the year. Consider that:
    \[
    M\xi_1 = m_1, \; M\xi_2 = m_2 ; \;\;
    \sigma_1^2 = \mathcal{D}\xi_1, \; \sigma_2^2 = \mathcal{D}\xi_2; \;\;
    \rho(\xi_1, \xi_2) = \rho.
    .\] 
    Consider linear prediction:
    \[
        \widetilde{\xi_2} = a\xi_1 + b
    .\] 
    where $a, b$ are some constants.
    Find $a, b$ from the condition of minimization of standard deviation $\widetilde{\xi_2}$ and $\xi_2$, otherwords:
    \[ M(\widetilde{\xi_2} - \xi_2)^2 \rightarrow min \]
    Calcualte $M(\widetilde{\xi_2} - \xi_2)^2$:
            \begin{gather*}
                M(\widetilde{\xi_2} - \xi_2) = \mathcal{D}(\widetilde{\xi_2} - \xi_2) + (M(\widetilde{\xi_2} - \xi_2))^2 = \mathcal{D}\widetilde{\xi_2} \mathcal{D}\xi_2 - 2\cov(\widetilde{\xi_2}, \xi_2) + (M \widetilde{\xi_2} - M\xi_2)^2 =\\
                = a^2 \cdot \mathcal{D}\xi_1 + \mathcal{D}\xi_2 - 2a\cdot \cov(\xi_1, \xi_2)
                    + (aM\xi_1 + b-M\xi_2)^2 = \\
                ( a^2\cdot \sigma_1^2 + \sigma_2^2 - 2a \rho \sigma_1 \cdot \sigma_2 ) + 
                    (am_1 + b - m_2) ^2
            \end{gather*}
            \[ (am_1 + b -m_2)^2 \geq 0 \]
            Consider $am_1 + b - m_2 = 0; \;\; b = m_2 - am_1$
            Minimize first part $a^2\cdot \sigma_1^2 + \sigma_2^2 - 2a \rho \sigma_1 \cdot \sigma_2$:
            \begin{gather*}
                2a\cdot \sigma_1^2 - 2\rho\sigma_1\sigma_2 = 0 \\
                a = \rho \frac{\sigma_2}{\sigma_2} \\
                2\sigma_1^2 > 0 
            \end{gather*}
            So the best predition:
            \[
                \widetilde{\xi_2} = \rho \frac{\sigma_2}{\sigma_1}\cdot \xi_1 + (m_2 - \rho\frac{\sigma_2}{\sigma_1} m_1) = \rho\frac{\sigma_2}{\sigma_1}(\xi_1 - m_1) + m_2;
            .\] 
            \[
                M(\widetilde{\xi_2} - \xi_2)^2 = \sigma_2^2(1-\rho^2).
            .\] 
            If $|\rho| = 1$ then $M(\widetilde{\xi_2} - \xi_2)^2 = 0 \Rightarrow \text{ min prediction}$
            \[
                \widetilde{\xi_2} = \rho\frac{\sigma_2}{\sigma_1} (\xi_1 - m_1) + m_2
            .\] is precise.

            If $|\rho| = 0 \Rightarrow M(\widetilde{\xi_2} - \xi_2)^2 = \sigma^2$ and $\widetilde{\xi_2} = m_2$ does not depend on $\xi_1$.
\end{example}

\section{Equation of full probability for expectation}

\begin{example}
    Firstly, dices are rolled, then a coin is flipped times the points on dice. How to find expectation of tails number?

    Let $\xi$ - number of points of dice rolled.

    $\eta$ - number of tails within $\xi$ flips.
    \[
        \eta = \sum_{i=1}^{\xi} \mathds{1}(\text{tail within i-th flip})
    .\] - number of tails.

    Number of additions is random.

    (will continue soon...)
\end{example}

\begin{definition}
    Probability distribution:
    \[
        P(\xi = x_i \mid H ), i\geq 1
    .\] 
    is conditional distribution of discrete random value $\xi$ within $H$, where $H$ is random event, $P(H) > 0$.
\end{definition}
\begin{definition}
    Conditional expectation of random value $\xi$ within $H$ is
    \[
        M(\xi \mid H) = \sum _{i=1}^{\infty} x_i \cdot P(\xi = x_i \mid H)
    .\]
\end{definition}
\begin{theorem}[formula of full probability]
    Let $\xi$ random value; $\set{H_1, \dots, H_n}$ - full group of events. Then:
    \[
        M\xi = \sum_{i=1}^{n} P(H_i) \cdot M(\xi \mid H_i)
    .\] 

    \begin{proof}
        \begin{gather*}
            \sum_{i=1}^{n} P(H_i) \cdot M(\xi \mid H_i) =
            \sum_{i=1}^{n}P(H_i) \cdot \sum_{j=1}^{m} x_j \cdot P(\xi = x_j \mid H_i) = \\
            \sum_{i=1}^{n}\sum_{j=1}^{m}x_j \cdot P(H_i) \cdot \frac{P(\xi = x_i, H_i)}{P(H_i)} =
            \sum_{j=1}^{m}x_j \cdot \sum_{i=1}^{n}P(\xi = x_i, H_i) = \\
            \sum_{j=1}^{n}x_j \cdot P(\xi = x_j) = M\xi
        \end{gather*}
    \end{proof}
\end{theorem}

\begin{example}[continuation]
    \begin{gather*}
        \eta = \sum_{i=1}^{\xi} \mathds{1}(\text{tails in i-th attempt}) \\
        M\eta = \sum_{k=1}^{6} P(\xi=k) M[\eta \mid \xi = k] =
        \sum_{k=1}^{6}\frac{1}{6} \cdot M\sum_{i=1}^{k} \mathds{1}(\text{tail on i-th attempt}) = \\
        = \sum_{k=1}^{6} \frac{1}{6} \cdot \sum_{i=1}^{k}  P(\text{tails on i-th attempt}) = 
        \frac{1}{6}\sum_{k=1}^{6} k \cdot \frac{1}{2} = \frac{1}{6} \cdot \frac{1}{2} \cdot \frac{1+6}{2} \cdot 6 = \frac{7}{4} = 1.75.
    \end{gather*}
\end{example}

\begin{example}
    Let $\xi \sim \text{Poiss}(\lambda)$; $x_1, x_2, \dots$ - independent equally distributed,
    $x_i \sim \text{Exp}(\alpha)$.
    \begin{gather*}
        \eta = \sum_{i=1}^{\xi} P(\xi=k) \cdot M[\eta \mid \xi = k] =
        \sum_{k=0}^{\infty} P(\xi=k)\cdot M\sum_{i=1}^{k} x_i =
        \sum_{k=0}^{\infty} P(\xi=k) \cdot \sum_{i=1}^{k}Mx_i = \\
        Mx_i = Mx_2 = \dots = Mx_k \\
        = \sum_{k=0}^{\infty}P(\xi_k) \cdot k \cdot Mx_1 = M\xi \cdot Mx_1
    \end{gather*}
    \[
    M\eta = \frac{\lambda}{\alpha}
    .\] 
\end{example}

\section{Inequalities related to moments of random values}
\subsection{Chebyshev inequality}

Let $\xi$ - integral (невід'ємна) random value. Then:
\[
    \forall \varepsilon > 0 : P(\xi \geq \varepsilon) \leq \frac{M\xi}{\varepsilon}
.\] 
\begin{proof}
    \begin{gather*}
        \xi - \xi \cdot \mathds{1}(\xi \geq \varepsilon) + \xi \cdot \mathds{1}(\xi < \varepsilon) \geq
        \xi \cdot \mathds{1} (\xi \geq \varepsilon) \geq \varepsilon \cdot \mathds{1}(\xi \geq \varepsilon) \Rightarrow \\
        \Rightarrow M\xi \geq M(\varepsilon \cdot \mathds{1}(\xi \geq \varepsilon)) =
        \varepsilon \cdot P(\xi \geq \varepsilon) \Rightarrow \\
        \Rightarrow P(\xi \geq \varepsilon) \leq \frac{M\xi}{\varepsilon}
    \end{gather*}
\end{proof}

\begin{corollary}
    \begin{enumerate}
        \item If $\xi$ - arbitrary random value, then 
            \[
                P(|\xi| \geq \varepsilon) \leq \frac{M|\xi|}{\varepsilon}
            .\] 
        \item $P(|\xi| \geq \varepsilon) = P(|\xi|^k \geq \varepsilon^k) \leq \frac{M|\xi|^k}{\varepsilon^k} $
        \item $P(|\xi - M\xi| \geq \varepsilon) \leq \frac{\mathcal{D}\xi}{\varepsilon^2}$

            Indeed:
            \begin{gather*}
                P(|\xi -M\xi| \geq \varepsilon) = P(|\xi - M\xi|^2 \geq \varepsilon^2) \leq \frac{M(\xi - M\xi)srJ}{\varepsilon^2} = \frac{\mathcal{D}\xi}{\varepsilon^2}
            \end{gather*}
    \end{enumerate}
\end{corollary}

\begin{example}
    \begin{enumerate}
        \item Rule of "three sigm"

            Let $\xi$ random value with expectation $M\xi$ and $\mathcal{D}\xi = \sigma^2$;

            $\sigma = \sqrt{\mathcal{D}\xi}$ - standard deviation of random value.
            \begin{gather*}
                P(|\xi - M\xi| > 3 \sigma) \leq \frac{\mathcal{D}\xi}{9\sigma^2} = \frac{\sigma^2}{ 0 \sigma^2} = \frac{1}{9} \Rightarrow \\ 
                P(|\xi - M\xi| < 3\sigma) \geq 1 - \frac{1}{9}.
            \end{gather*}
            If $\xi_1, \xi_2 \dots, \xi_N$ independent equally distributed random values, then at least $90\%$ of observations will be in interval:
            \[
                (m - 3\sigma, m + 3\sigma)
            .\] where $m = M\xi_1$.

            $\xi_1, \xi_2, \dots, \xi_N \sim N(0,1)$ independent.
            Then $\approx 90\%$ will be in interval $(-3, 3)$.

        \item Let $p$ - unknown part of population of a country support some resolution. For definition $p$ is used social poll.

            $n$ persons are polled:
            \[
                \sum_{i=1}^{n}\mathds{1}(\text{i-th person support the resolution})
            .\] 
            $\frac{S_n}{n}$ - part of those, who support the resolution.

            $\frac{S_n}{n} \approx p$ - within large $n$.

            The question is, how large must be $n$ for the deviation $\frac{S_n}{n}$ to be quite small.
            For instance:
            \[
                P\left( \left| \frac{S_n}{n} - p \right| \geq 0.1 \right) \leq 0.05
            .\] 
            Notice that:
            \[
                M\left( \frac{S_n}{n} \right) = \frac{1}{n} \cdot MS_n = \frac{1}{n} \cdot np = p.
            .\] 
            \[
                P \left( \left| \frac{S_n}{n} - M \left(\frac{S_n}{n}\right) \right| \leq 0.1 \right) \leq \frac{\mathcal{D} \left(\frac{S_n}{n}\right)}{(0.1)^2} = \frac{\mathcal{D}S_n}{n^2 \cdot (0.1)^2} = \frac{n\cdot p \cdot (1-p)}{n^2 \cdot (0.1)^2} = \frac{p(1-p)}{n(0.1)^2} \leq
            \] 
            \[
                \leq \frac{1}{4n (0.1)^2} \;\; \text{ as }  \;\; 
                \forall p \in (0,1): p(1-p) \leq \frac{1}{4}
            .\] 
            Find $n$ from the condition:
            \[
                \frac{1}{4n(0.1)^2} \leq 0.05 \Rightarrow n \geq \frac{1}{4 \cdot 0.05 \cdot (0.1)^2}
            .\] 
    \end{enumerate}
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   Inequalities. The law of large numbers in the form of Chebyshev.
%%%   Borel-Cantelli lemma
%%%   Lection 12
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Inequalities. The law of large numbers in the form of Chebyshev.
Borel-Cantelli lemma}

\section{Cauchy-Bunyakovsky inequality}

Let $\xi, \eta$ - stochastic variables such that $M\xi^2 < \infty, M\eta^2 < \infty$. Then $M|\xi \eta | < \infty$ and
\begin{equation}
    M|\xi \cdot \eta| \leq \sqrt{M\xi^2} \cdot \sqrt{M\eta^2}
\end{equation}

\begin{proof}
    \[
        \widetilde{\xi} \equiv \frac{\xi}{\sqrt{M\xi^2}}, \; \widetilde{\eta} \equiv \frac{\eta}{\sqrt{M\eta^2}}
    .\] 
    Whereas \[ (|\widetilde{\xi}| - |\widetilde{\eta}|)^2 \geq 0 \], then 
\[ 2 |\widetilde{\xi}| \cdot |\widetilde{\eta}| \leq \widetilde{\xi}^2 + |\widetilde{\eta}|^2 \]
   Take expectation:
   \[
       2 M[|\widetilde{\xi}| \cdot |\widetilde{\eta}| ] \leq M \widetilde{\xi}^2 + M|\widetilde{\eta}|^2 = 2
   \] 
   \[ \Rightarrow M[|\widetilde{\xi}|\cdot |\widetilde{\eta}| ] \leq 1 \]
   \[ \Rightarrow M |\xi| \cdot |\eta| \leq \sqrt{M\xi^2} \cdot \sqrt{M\eta^2} \]
\end{proof}

\section{Jensen's inequality}
Let $g(x)$ convex downward Borel function (опукла донизу борелівська функція) and $M|\xi| < \infty$. Then
\[
    g(M\xi) \leq Mg(\xi)
.\] 
\begin{proof}
    If $g$ is convex downward, then
    \[
        \forall x_0 \in \mathbb{R} \; \exists \lambda = \lambda(x_0) : g(x) \geq g(x_0) + (x - x_0)\cdot \lambda
    .\] 
    Consider $x = \xi, x_0 = M\xi$. Got 
    \[
        g(\xi) \geq g(M\xi) + (\xi - M\xi)\cdot \lambda
    .\] 
    Apply expectation:
    \[ Mg(\xi) \geq Mg(M\xi) + \lambda \cdot M(\xi - M\xi) \] 
    \[ Mg(M\xi) = const; \;\; M(\xi - M\xi) = 0\] 
    \[ \Rightarrow Mg(\xi) \geq g(M(\xi)) \]
\end{proof}

\section{Lyapunov inequality}
If $0 < s < t$, then
\[
\left( M|\xi|^{s} \right)^{1/s} \leq \left( M|\xi|^{t} \right) ^{1/t}
.\] 
\begin{proof}
    
\end{proof}

\begin{corollary}
   \[
       M|\xi| \leq \left(M|\xi|^2\right)^{1/2} \leq \left(M|\xi|^{3}\right)^{1/3} \leq \dots \leq \left(M|\xi|^{n}\right)^{1/n}
   .\]  
\end{corollary}

\section{Helder inequality}
Let $1 < p < \infty$ and $1 < q < \infty$, $\frac{1}{p} + \frac{1}{q} = 1$.

If $M|\xi|^{p} < \infty$, $M|\eta|^{q} < \infty$, then
\[
    M|\xi \eta | < \infty \;\; \text{ and } \;\; M|\xi \cdot \eta | \leq \left( M|\xi|^{p} \right)^{1/p} \cdot \left( M|\eta|^{q} \right)^{1/q}
.\] 
(at $p = q = 2$ we obtain Cauchy-Bunyakovsky inequality)
\begin{proof}
    Let $\tilde{\xi} = \frac{\xi}{\left(M|\xi|^{p}\right)^{1/p}}$; $\tilde{\eta} = \frac{\eta}{\left(M|\eta|^{q}\right)^{1/q}}$.
    Function $\ln x$ is convex upward. That's why we have $\forall a, b > 0$ and $a + b = 1$ within $x, y > 0$:
    \[ \ln(ax + by) \geq a \ln x \cdot b \ln y = \ln x^a y^b \]
    \[ \Rightarrow ax + by \geq x^a y^b \]

    Let $x = |\tilde{\xi}|^p$, $y = |\tilde{\eta}|^q$, $a = \frac{1}{p}$, $b = \frac{1}{q}$.
    Then got:
    $$
    \begin{array}{l}
    |\tilde{\xi} \cdot \tilde{\eta}| \leqslant\frac{1}{p}\left|\tilde{\xi}\right|^{p} + \frac{1}{q}\left| \tilde{\eta}\right|^{q} \\
        M|\tilde{\xi} \cdot \tilde{\eta}| \leqslant \frac{1}{p} \underbrace{M|\tilde{\xi}|^{p}}_{= 1}+\frac{1}{q} \underbrace{M|\tilde{\eta}|^{q}}_{=1} =1 \\
    M|\xi \cdot \eta| \leqslant\left(M|\xi|^{p}\right)^{1 / p}\left(M|\eta|^{q}\right)^{1 / q}
    \end{array}
    $$
    
\end{proof}

\section{Minkovkiy inequality}

If $M|\xi|^{p} < \infty$, $M|\eta| ^{p} < \infty$, $1 \leq p < \infty$, then 
\[
M|\xi + \eta| ^{p} < \infty
\] 
and
\[
\left( M|\xi + \eta|^{p} \right)^{1/p} \leq \left( M|\xi|^{p} \right)^{1/p} + \left( M|\eta|^{p} \right)^{1/p}
.\] 
\begin{proof}
    
\end{proof}

\section{The law of large numbers in the form of Chebyshev}

Let $X_1, X_2, \dots$ - sequence of random values with finite expectation $m_i = MX_i$.

\begin{definition}
    The sequence $\set{X_n}_{n\geq 1}$ satisfies the law of large numbers, if 
    \[
        \forall \varepsilon > 0 : P \left( \left| \frac{1}{n} \sum_{i=1}^{n}X_i - \frac{1}{n}\sum_{i=1}^{n}m_i \right| \geq \varepsilon \right) \underset{n \rightarrow \infty}{\longrightarrow} 0
    .\] 
\end{definition}

\begin{theorem}[The Law of large numbers in the form of Chebyshev]
    Let $\set{x_n}_{n\geq1}$ sequence of independent random values that have finite expectations $Mx_i = m_i$ and $\mathcal{D}x_I = \sigma_i^{2}$, moreover the dispersions are evenly limited:
    \[
    \forall i \;\; \sigma_i^2 \leq C < \infty
    .\] 
    then $\set{x_n}_{n\geq1}$ satisfies the law of large numbers.
    
\end{theorem}X-

\begin{corollary}
    For independent evenly distributed random values:

    if $\set{x_n}_{n\geq1}$ sequence with finite $m = Mx_1, \sigma^2 = \mathcal{D}x_1$ then
    \[
        \forall \varepsilon > 0 \;\; P \left( \left| \frac{1}{n}\sum_{i=1}^{n} x_i - m \right| \geq \varepsilon \right) \underset{n\to\infty}{\longrightarrow} 0
    .\] 
\end{corollary}
\begin{remark}
    For use of the law of large numbers the finiteness of expectation is enough (will be proven later).
\end{remark}

\begin{example}
    \begin{itemize}
        \item Bernoulli theorem:
            \begin{theorem}[Bernoulli]
                Let $S_n$ be a number of <<successes>> in $n$ unrelated repeated trials with probability of <<success>> $p$ in each one. Then
                \[ \forall \varepsilon > 0 : P \left( \left| \frac{S_n}{n} - p \right| \geq \varepsilon \right) \underset{n \to \infty}{\longrightarrow} 0 \]
                Indeed --- $ S_n = \sum\limits_{i=1}^{n} X_i $ , where
                $X_i \sim B(p); \; X_i $ are independent;

                $M X_i = p; \; \mathcal{D} X_i = p(1-p) \leq \frac{1}{4} $.
            \end{theorem}
        \item Poisson theorem
            \begin{theorem}[Poisson]
                Let $S_n$ be a number of successes in $n$ trials. In $k$-th trials the probability of success in $p_k$.
                Then:
                \[
                    \forall \varepsilon > 0 : P \left( \left| \frac{S_n}{n} - \frac{1}{n} \sum_{i=1}^{n}p_i \right| \geq \varepsilon \right) \underset{n\to\infty}{\longrightarrow} 0
                \]
                Indeed: $ S_n = \sum\limits_{i=1}^{n} X_i $ ,\\
                $X_i \sim B(p_i); \; MX_i = p_i; \; \mathcal{D}X_i = p_i (1 - p_i) \leq \frac{1}{4}$
            \end{theorem}
                
    \end{itemize}
\end{example}

\section{Borel-Cantelli}
The next lemma is the main tool for analysis of properties with probability of 1.

Consider $\set{A_n}_{n\geq}$ - a sequence of random events from $\sigma$-algebra $\mathcal{F}$.

Call to mind next notation:
\[ \overline{\lim\limits_{n\to \infty}} A_n = \set{A_n \text{ occurs for infinitely many $n$}} \equiv \set{A_n \text{ i.o. (infinitely often)}} = \]
\[ = \bigcap\limits_{n=1}^{\infty} \bigcup_{k\geq n} A_k \]
\[ \forall n \; \exists k \geq n : A_k \text{ occurred } \]

\[ \underset{n \to \infty }{\underline{\lim}} A_n = \set{\text{ from some number all the $A_n$ events occur }} =
    \bigcup_{n=1}^{\infty} \bigcap_{k\geq n} A_k
\]
\[ (\exists n \; \forall k \geq n A_k \text{ occurs }) \]
\begin{lemma}[Borel-Cantelli] Got several cases:
    \begin{itemize}
        \item[a.] If $\sum\limits_{n=1}^{\infty} P(A_n) < \infty$, then $P(A_n \text{ i.o. }) = 0$.
    \item[b.] If $\sum\limits_{n=1}^{\infty} P(A_n) = \infty$ and $A_1, A_2, \dots$ are independent, then $P(A_n \text{ i.o. }) = 1$.
    \end{itemize}

    \begin{proof}
        \begin{itemize}
            \item[a.] By definition:
                \[ P \left( \underset{n \to \infty}{\overline{\lim}} A_n \right) =
                P \left( \bigcap_{n=1}^{\infty} \bigcup_{k\geq n}^{} A_k \right) \]
                Create the following subsequence:
                \[ B_1 = \bigcup_{k=1}^{\infty} A_k \supset B_2 = \bigcup_{k=2}^{\infty} A_k \supset \dots \]
                The $P \left( \bigcap_{n=1}^{\infty} \bigcup_{k\geq n}^{} A_k \right)$ is continuous from above:
                \[ P \left( \bigcap_{n=1}^{\infty} \bigcup_{k\geq n}^{} A_k \right) = \underset{n\to\infty}{\lim} P \left( \bigcup_{k\geq n} A_k \right) \leq \underset{n\to \infty}{\lim} \sum_{k\geq n} P(A_k) = 0 \]
                as long as the series $\sum\limits_{k=1}^{\infty} P(A_k)$ is convergent.
                
        \end{itemize}
    \end{proof}
\end{lemma}


\end{document}

